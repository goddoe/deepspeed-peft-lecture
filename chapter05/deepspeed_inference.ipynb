{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f607d7e0-3a84-4ed4-a1fd-be311ce7181b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages (from seqeval) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages (from seqeval) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=507d0e9a6dd688e32bf43611779cb32f8b7eca3bafcf2c24e4639cbcf9bf5458\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-nq_9wlbk/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c794ae93-6f21-4ed9-9b08-f235e569fd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20394e7d-293b-4e5f-aedf-d1608f4142b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40.0/40.0 [00:00<00:00, 73.7kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.45k/1.45k [00:00<00:00, 4.01MB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 213k/213k [00:00<00:00, 603kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 516kB/s]\n",
      "Downloading pytorch_model.bin: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.33G/1.33G [01:00<00:00, 22.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.9971493, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}, {'entity': 'B-LOC', 'score': 0.99860424, 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "# Model Repository on huggingface.co\n",
    "model_id=\"dslim/bert-large-NER\"\n",
    "\n",
    "# Load Model and Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_id)\n",
    "\n",
    "# Create a pipeline for token classification\n",
    "token_clf = pipeline(\"token-classification\", model=model, tokenizer=tokenizer,device=0)\n",
    "\n",
    "# Test pipeline\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "ner_results = token_clf(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8417ffff-694a-4170-8f0b-c58c2715cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluator\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d18ca4-1655-458f-833a-d1df632b66ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9.57k/9.57k [00:00<00:00, 8.48MB/s]\n",
      "Downloading metadata: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3.73k/3.73k [00:00<00:00, 10.6MB/s]\n",
      "Downloading readme: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12.3k/12.3k [00:00<00:00, 9.27MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset conll2003/conll2003 to /home/nsml/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 983k/983k [00:01<00:00, 644kB/s]\n",
      "                                                                                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset conll2003 downloaded and prepared to /home/nsml/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6.34k/6.34k [00:00<00:00, 5.55MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall f1 score for our model is 95.76%\n",
      "The avg. Latency of the model is 13.37ms\n"
     ]
    }
   ],
   "source": [
    "# load eval dataset\n",
    "eval_dataset = load_dataset(\"conll2003\", split=\"validation\")\n",
    "\n",
    "# define evaluator\n",
    "task_evaluator = evaluator(\"token-classification\")\n",
    "\n",
    "# run baseline\n",
    "results = task_evaluator.compute(\n",
    "    model_or_pipeline=token_clf,\n",
    "    data=eval_dataset,\n",
    "    metric=\"seqeval\",\n",
    ")\n",
    "\n",
    "print(f\"Overall f1 score for our model is {results['overall_f1']*100:.2f}%\")\n",
    "print(f\"The avg. Latency of the model is {results['latency_in_seconds']*1000:.2f}ms\")\n",
    "# Overall f1 score for our model is 95.76%\n",
    "# The avg. Latency of the model is 13.37ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cda24dce-28c3-4fe8-81e0-cdcfaf28bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification,pipeline\n",
    "from transformers import pipeline\n",
    "from deepspeed.module_inject import HFBertLayerPolicy\n",
    "import deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "392b3ce4-6e46-41a1-aec3-74cf8afeeda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-03 03:05:02,141] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.2, git-hash=unknown, git-branch=unknown\n",
      "[2023-06-03 03:05:02,145] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
      "[2023-06-03 03:05:02,145] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2023-06-03 03:05:02,146] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/nsml/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Creating extension directory /home/nsml/.cache/torch_extensions/py311_cu117/transformer_inference...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/nsml/.cache/torch_extensions/py311_cu117/transformer_inference/build.ninja...\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/TH -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/nsml/.conda/envs/fc_peft/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -c /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/relu.cu -o relu.cuda.o \n",
      "[2/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/TH -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/nsml/.conda/envs/fc_peft/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -c /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/dequantize.cu -o dequantize.cuda.o \n",
      "[3/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/TH -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/nsml/.conda/envs/fc_peft/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -c /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/apply_rotary_pos_emb.cu -o apply_rotary_pos_emb.cuda.o \n",
      "[4/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/TH -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/nsml/.conda/envs/fc_peft/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -c /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu -o transform.cuda.o \n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(63): warning #177-D: variable \"lane\" was declared but never referenced\n",
      "\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(103): warning #177-D: variable \"half_dim\" was declared but never referenced\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int) [with T=__nv_bfloat16]\" \n",
      "(244): here\n",
      "\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(120): warning #177-D: variable \"vals_half\" was declared but never referenced\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int) [with T=__nv_bfloat16]\" \n",
      "(244): here\n",
      "\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(121): warning #177-D: variable \"output_half\" was declared but never referenced\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int) [with T=__nv_bfloat16]\" \n",
      "(244): here\n",
      "\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(138): warning #177-D: variable \"lane\" was declared but never referenced\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int) [with T=__nv_bfloat16]\" \n",
      "(244): here\n",
      "\n",
      "[5/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/TH -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/nsml/.conda/envs/fc_peft/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -c /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/gelu.cu -o gelu.cuda.o \n",
      "[6/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/TH -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/nsml/.conda/envs/fc_peft/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -c /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/softmax.cu -o softmax.cuda.o \n",
      "[7/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/TH -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/nsml/.conda/envs/fc_peft/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -c /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/layer_norm.cu -o layer_norm.cuda.o \n",
      "[8/9] c++ -MMD -MF pt_binding.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/TH -isystem /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/nsml/.conda/envs/fc_peft/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++14 -g -Wno-reorder -c /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp -o pt_binding.o \n",
      "In file included from /home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:10:\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/includes/inference_context.h: In member function ‘void InferenceContext::GenWorkSpace(const unsigned int&, const unsigned int&, const size_t&, const size_t&, const size_t&, const unsigned int&, const bool&, const size_t&, const unsigned int&, unsigned int, unsigned int)’:\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/includes/inference_context.h:139:52: warning: format ‘%d’ expects argument of type ‘int’, but argument 2 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "  139 |                 \"Allocatable workspace available (%d tokens) is less than minimum requested \"\n",
      "      |                                                   ~^\n",
      "      |                                                    |\n",
      "      |                                                    int\n",
      "      |                                                   %ld\n",
      "  140 |                 \"workspace (%d tokens)\\n\",\n",
      "  141 |                 _max_seq_len,\n",
      "      |                 ~~~~~~~~~~~~                        \n",
      "      |                 |\n",
      "      |                 size_t {aka long unsigned int}\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of ‘std::vector<at::Tensor> ds_softmax_context(at::Tensor&, at::Tensor&, int, bool, bool, int, float, bool, bool, int, bool, unsigned int, unsigned int, at::Tensor&) [with T = float]’:\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1809:5:   required from here\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:536:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  536 |                                      {hidden_dim * InferenceContext::Instance().GetMaxTokenLenght(),\n",
      "      |                                       ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:536:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:537:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  537 |                                       k * InferenceContext::Instance().GetMaxTokenLenght(),\n",
      "      |                                       ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:537:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:545:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  545 |                          {hidden_dim * InferenceContext::Instance().GetMaxTokenLenght(),\n",
      "      |                           ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:545:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:546:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  546 |                           k * InferenceContext::Instance().GetMaxTokenLenght(),\n",
      "      |                           ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:546:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of ‘std::vector<at::Tensor> ds_softmax_context(at::Tensor&, at::Tensor&, int, bool, bool, int, float, bool, bool, int, bool, unsigned int, unsigned int, at::Tensor&) [with T = __half]’:\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1810:5:   required from here\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:536:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  536 |                                      {hidden_dim * InferenceContext::Instance().GetMaxTokenLenght(),\n",
      "      |                                       ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:536:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:537:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  537 |                                       k * InferenceContext::Instance().GetMaxTokenLenght(),\n",
      "      |                                       ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:537:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:545:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  545 |                          {hidden_dim * InferenceContext::Instance().GetMaxTokenLenght(),\n",
      "      |                           ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:545:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:546:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  546 |                           k * InferenceContext::Instance().GetMaxTokenLenght(),\n",
      "      |                           ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:546:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "[9/9] c++ pt_binding.o gelu.cuda.o relu.cuda.o layer_norm.cuda.o softmax.cuda.o dequantize.cuda.o apply_rotary_pos_emb.cuda.o transform.cuda.o -shared -lcurand -L/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o transformer_inference.so\n",
      "Time to load transformer_inference op: 32.11245942115784 seconds\n",
      "[2023-06-03 03:05:34,748] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1024, 'intermediate_size': 4096, 'heads': 16, 'num_hidden_layers': -1, 'fp16': True, 'pre_layer_norm': False, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-12, 'mp_size': 1, 'q_int8': False, 'scale_attention': True, 'triangular_masking': False, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False}\n",
      "Time to load transformer_inference op: 0.0033452510833740234 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module transformer_inference...\n",
      "Using /home/nsml/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "The model 'InferenceEngine' is not supported for token-classification. Supported models are ['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'BioGptForTokenClassification', 'BloomForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification', 'ConvBertForTokenClassification', 'Data2VecTextForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'ErnieForTokenClassification', 'ErnieMForTokenClassification', 'EsmForTokenClassification', 'FlaubertForTokenClassification', 'FNetForTokenClassification', 'FunnelForTokenClassification', 'GPT2ForTokenClassification', 'GPT2ForTokenClassification', 'GPTBigCodeForTokenClassification', 'GPTNeoForTokenClassification', 'GPTNeoXForTokenClassification', 'IBertForTokenClassification', 'LayoutLMForTokenClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv3ForTokenClassification', 'LiltForTokenClassification', 'LongformerForTokenClassification', 'LukeForTokenClassification', 'MarkupLMForTokenClassification', 'MegaForTokenClassification', 'MegatronBertForTokenClassification', 'MobileBertForTokenClassification', 'MPNetForTokenClassification', 'NezhaForTokenClassification', 'NystromformerForTokenClassification', 'QDQBertForTokenClassification', 'RemBertForTokenClassification', 'RobertaForTokenClassification', 'RobertaPreLayerNormForTokenClassification', 'RoCBertForTokenClassification', 'RoFormerForTokenClassification', 'SqueezeBertForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaXLForTokenClassification', 'XLNetForTokenClassification', 'XmodForTokenClassification', 'YosoForTokenClassification'].\n",
      "Loading cached processed dataset at /home/nsml/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-d8ab495eecd30d6d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Free memory : 76.731995 (GigaBytes)  \n",
      "Total memory: 79.210449 (GigaBytes)  \n",
      "Requested memory: 0.144531 (GigaBytes) \n",
      "Setting maximum total tokens (input + output) to 1024 \n",
      "WorkSpace: 0x7f3612000000 \n",
      "------------------------------------------------------\n",
      "[{'entity': 'B-PER', 'score': 0.9971342, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}, {'entity': 'B-LOC', 'score': 0.9985997, 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}]\n",
      "Overall f1 score for our model is 95.76%\n",
      "The avg. Latency of the model is 6.73ms\n"
     ]
    }
   ],
   "source": [
    "# Model Repository on huggingface.co\n",
    "model_id=\"dslim/bert-large-NER\"\n",
    "\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_id)\n",
    "\n",
    "# init deepspeed inference engine\n",
    "ds_model = deepspeed.init_inference(\n",
    "    model=model,      # Transformers models\n",
    "    mp_size=1,        # Number of GPU\n",
    "    dtype=torch.half, # dtype of the weights (fp16)\n",
    "    # injection_policy={\"BertLayer\" : HFBertLayerPolicy}, # replace BertLayer with DS HFBertLayerPolicy\n",
    "    replace_method=\"auto\", # Lets DS autmatically identify the layer to replace\n",
    "    replace_with_kernel_inject=True, # replace the model with the kernel injector\n",
    ")\n",
    "\n",
    "# create acclerated pipeline\n",
    "ds_clf = pipeline(\"token-classification\", model=ds_model, tokenizer=tokenizer,device=0)\n",
    "\n",
    "# Test pipeline\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "ner_results = ds_clf(example)\n",
    "print(ner_results)\n",
    "\n",
    "\n",
    "# run baseline\n",
    "ds_results = task_evaluator.compute(\n",
    "    model_or_pipeline=ds_clf,\n",
    "    data=eval_dataset,\n",
    "    metric=\"seqeval\",\n",
    ")\n",
    "\n",
    "print(f\"Overall f1 score for our model is {ds_results['overall_f1']*100:.2f}%\")\n",
    "print(f\"The avg. Latency of the model is {ds_results['latency_in_seconds']*1000:.2f}ms\")\n",
    "# Overall f1 score for our model is 95.76%\n",
    "# The avg. Latency of the model is 6.73ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8840bb6a-0998-4f34-b55a-215bbad19087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f8ec0-06d9-4879-9a0a-e5d10549982f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
