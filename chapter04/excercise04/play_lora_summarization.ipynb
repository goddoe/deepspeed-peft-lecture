{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b06fff7-4e38-4bd6-8af0-34a0740e72c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dced701e-2f20-4a1b-bac5-54b7ccaa0335",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./outputs/lora/checkpoint-8325\"\n",
    "config = PeftConfig.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "391aeec2-b1e4-4c75-9e7a-bc46ad544629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTNeoXForCausalLM(\n",
       "      (gpt_neox): GPTNeoXModel(\n",
       "        (embed_in): Embedding(30080, 3072)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x GPTNeoXLayer(\n",
       "            (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): GPTNeoXAttention(\n",
       "              (rotary_emb): RotaryEmbedding()\n",
       "              (query_key_value): Linear(\n",
       "                in_features=3072, out_features=9216, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (dense): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "            )\n",
       "            (mlp): GPTNeoXMLP(\n",
       "              (dense_h_to_4h): Linear(in_features=3072, out_features=12288, bias=True)\n",
       "              (dense_4h_to_h): Linear(in_features=12288, out_features=3072, bias=True)\n",
       "              (act): GELUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (embed_out): Linear(in_features=3072, out_features=30080, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_model = PeftModel.from_pretrained(model, model_path)\n",
    "inference_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "111277ae-08f8-4a04-b6c3-fd3d315a877c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=inference_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7468d1d2-6dd2-4f02-b050-fc11564ae681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(title, doc):\n",
    "    return f\"다음의 문서를 요약하시오\\n제목: {title}\\n본문: {doc}\\n요약:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65b48238-48a5-4d2a-a17e-84cd14861fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"일론 머스크, '세계 최고 부자' 타이틀 탈환... 순자산 253조원\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "412d6f47-ff04-48bf-b65d-8e9295a2d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "일론 머스크(54) CEO가 다시 한번 세계 최고 부자 타이틀을 되찾았다.\n",
    "앞서 전기차 업체 ‘테슬라’ 주가가 급락하면서 명품 브랜드 그룹 ‘루이뷔통모에헤네시(LVMH)’의 베르나르 아르노(74) 회장에게 내줬던 자리를 다시 탈환한 것이다.\n",
    "테슬라의 주가가 상승세를 그리면서 머스크 CEO의 순자산은 올해 1월 이후 553억달러(약 72조원) 더 늘어나 1920억달러(약 253조원)를 기록 중이다.\n",
    "블룸버그가 집계하는 전 세계 억만장자 순위에 따르면 아르노 회장의 재산은 245억달러(약 32조원) 감소한 1870억달러(약 244조원) 에 머물렀다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1adaffa-d2f1-43d1-a1c5-b99981db3632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ' 일일론 머스크 CEO가 다시 한번 세계 최고 부자 타이틀을 되찾았는데, 올해 1월 이후 테슬라의 주가가 상승세를 그리면서 머스크 CEO의 순자산은 올해 1월 이후 553억달러 더 늘어나 1920억달러(약 253조원)를 기록 중이며, 아르노 회장의 재산은 245억달러(약 32조원) 감소한 1870억달러(약 244조원) 에 머물렀다.'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(build_prompt(title, doc),\n",
    "     do_sample=False,\n",
    "     max_new_tokens=128,\n",
    "     return_full_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1184c0e4-d337-4b24-be96-13978fdf1563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
