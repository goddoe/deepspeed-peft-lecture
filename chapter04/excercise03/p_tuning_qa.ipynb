{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37c11a69",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/goddoe/hacking-llms-for-low-res-settings/blob/main/p_tuning_qa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "012d8ec0-ac99-4776-acba-8329f0e07420",
   "metadata": {
    "id": "012d8ec0-ac99-4776-acba-8329f0e07420"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    TaskType,\n",
    "    PeftType,\n",
    "    PrefixTuningConfig,\n",
    "    PromptEncoderConfig,\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26744996-3c2c-4b04-8ffc-db4c09cc696e",
   "metadata": {
    "id": "26744996-3c2c-4b04-8ffc-db4c09cc696e"
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "model_name_or_path = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "peft_type = PeftType.PREFIX_TUNING\n",
    "device = \"cuda\"\n",
    "num_epochs = 5\n",
    "\n",
    "dataset_name = \"heegyu/korquad-chat-v1\"\n",
    "max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2d28f6-c637-48fb-a59b-6a44f1591d80",
   "metadata": {
    "id": "5d2d28f6-c637-48fb-a59b-6a44f1591d80"
   },
   "outputs": [],
   "source": [
    "peft_config = PromptEncoderConfig(task_type=TaskType.CAUSAL_LM,\n",
    "                                  num_virtual_tokens=20,\n",
    "                                  encoder_hidden_size=128)\n",
    "lr = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d61c7c84-46fa-41cb-a2ea-d01df62b2707",
   "metadata": {
    "id": "d61c7c84-46fa-41cb-a2ea-d01df62b2707"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e92454f-6d6f-461b-b65c-45cbab1193ba",
   "metadata": {
    "id": "6e92454f-6d6f-461b-b65c-45cbab1193ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/nsml/.cache/huggingface/datasets/heegyu___json/heegyu--korquad-chat-v1-aee7c160ef28202b/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 505.03it/s]\n",
      "Loading cached processed dataset at /home/nsml/.cache/huggingface/datasets/heegyu___json/heegyu--korquad-chat-v1-aee7c160ef28202b/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-76f46639e9b2e23a.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # max_length=None => use the model max length (it's actually the default)\n",
    "    outputs = tokenizer(examples[\"text\"], truncation=True, max_length=None)\n",
    "    return outputs\n",
    "\n",
    "def collate_fn(examples):\n",
    "    return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function,\n",
    "                                batched=True,\n",
    "                                remove_columns=[\"source\", \"text\"])\n",
    "\n",
    "td = tokenized_dataset['train'].train_test_split(train_size=0.8)\n",
    "\n",
    "train_dataloader = DataLoader(td['train'],\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=collate_fn)\n",
    "eval_dataloader = DataLoader(td['test'],\n",
    "                             batch_size=batch_size,\n",
    "                             collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24340647-2c04-434f-aa55-4289486b9128",
   "metadata": {
    "id": "24340647-2c04-434f-aa55-4289486b9128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 583936 || all params: 1332394240 || trainable%: 0.04382606757591507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsml/.conda/envs/fc_peft/lib/python3.11/site-packages/peft/tuners/p_tuning.py:146: UserWarning: for PromptEncoderReparameterizationType.MLP, the `encoder_num_layers` is ignored. Exactly 2 MLP layers are used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): GPTNeoXForCausalLM(\n",
       "    (gpt_neox): GPTNeoXModel(\n",
       "      (embed_in): Embedding(30080, 2048)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (embed_out): Linear(in_features=2048, out_features=30080, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PromptEncoder(\n",
       "      (embedding): Embedding(20, 2048)\n",
       "      (mlp_head): Sequential(\n",
       "        (0): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=128, out_features=2048, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(30080, 2048)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, return_dict=True)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "438f87d8-1539-4f84-8dc0-295b55903ebf",
   "metadata": {
    "id": "438f87d8-1539-4f84-8dc0-295b55903ebf"
   },
   "outputs": [],
   "source": [
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"], weight_decay=0.1):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    \n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            batch.to(device)\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "\n",
    "        losses.append(outputs.loss)\n",
    "    loss = torch.mean(torch.stack(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59f6e7ca-0207-4c7e-8912-7e03d94529ae",
   "metadata": {
    "id": "59f6e7ca-0207-4c7e-8912-7e03d94529ae"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(get_grouped_params(model), lr=lr)\n",
    "\n",
    "# Instantiate scheduler\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,  # 0.06*(len(train_dataloader) * num_epochs),\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b91afd1c-8c05-4141-b02f-8eda45580d30",
   "metadata": {
    "id": "b91afd1c-8c05-4141-b02f-8eda45580d30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                               | 0/7695 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7695/7695 [22:05<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'loss/eval': 1.8767658472061157, 'perplexity': 6.532344341278076}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7695/7695 [21:40<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: {'loss/eval': 1.8675352334976196, 'perplexity': 6.472324371337891}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7695/7695 [21:59<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: {'loss/eval': 1.8643139600753784, 'perplexity': 6.451508522033691}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7695/7695 [18:03<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: {'loss/eval': 1.8552756309509277, 'perplexity': 6.393460750579834}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7695/7695 [22:23<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: {'loss/eval': 1.845894455909729, 'perplexity': 6.333763122558594}\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "best_model_path = \"./outputs/best_p_tuning_model\"\n",
    "min_valid_ppl = 9999999.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch.to(device)\n",
    "        outputs = model(batch['input_ids'], labels=batch['input_ids'])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    eval_loss, perplexity = evaluate()\n",
    "    eval_metric = {\"loss/eval\": eval_loss, \"perplexity\": perplexity}\n",
    "\n",
    "    print(f\"epoch {epoch}:\", eval_metric)\n",
    "    if eval_metric['perplexity'] <= min_valid_ppl:\n",
    "        min_valid_ppl = eval_metric['perplexity']\n",
    "        model.save_pretrained(best_model_path)\n",
    "        tokenizer.save_pretrained(best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310d4201-37db-44e0-8aa8-a30f46a0f238",
   "metadata": {
    "id": "310d4201-37db-44e0-8aa8-a30f46a0f238"
   },
   "source": [
    "# Load and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfe5078e-765d-4ae8-9a6b-7bdc656d66b7",
   "metadata": {
    "id": "cfe5078e-765d-4ae8-9a6b-7bdc656d66b7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03e68b7c-8dbc-4e5c-8a6c-d810638fa6cd",
   "metadata": {
    "id": "03e68b7c-8dbc-4e5c-8a6c-d810638fa6cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): GPTNeoXForCausalLM(\n",
       "    (gpt_neox): GPTNeoXModel(\n",
       "      (embed_in): Embedding(30080, 2048)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (embed_out): Linear(in_features=2048, out_features=30080, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PromptEncoder(\n",
       "      (embedding): Embedding(20, 2048)\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(30080, 2048)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = PeftConfig.from_pretrained(best_model_path)\n",
    "inference_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "inference_model = PeftModel.from_pretrained(inference_model, best_model_path)\n",
    "inference_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "192eff45-2148-42a7-95be-2447fb436a15",
   "metadata": {
    "id": "192eff45-2148-42a7-95be-2447fb436a15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\",\n",
    "                     model=inference_model,\n",
    "                     tokenizer=tokenizer,\n",
    "                     device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54c2454b-89e4-4532-b5e3-8863b5140902",
   "metadata": {
    "id": "54c2454b-89e4-4532-b5e3-8863b5140902"
   },
   "outputs": [],
   "source": [
    "prompt = \"<sys>1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다.\\n<usr>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "950e4b24-2570-4e7b-bc0e-66ce2a0d0b91",
   "metadata": {
    "id": "950e4b24-2570-4e7b-bc0e-66ce2a0d0b91"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "bot_text = generator(f\"{prompt} 바그너가 1839년에 파우스트를 소재로 한 교향곡 작곡을 시작했다는데, 왜 이 소재에 마음이 끌렸을까?\\n<bot>\",\n",
    "                     max_new_tokens=128, \n",
    "                     return_full_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b80f2b05-779d-4aa5-8bb6-150c5a738cb6",
   "metadata": {
    "id": "b80f2b05-779d-4aa5-8bb6-150c5a738cb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': ' 바그너는 파우스트를 읽고 그 내용에 매료되었고, 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖게 되었다. 바그너는 파우스트를 읽고 그 내용에 매료되었고, 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖게 되었다.\\n<usr> 바그너는 교향곡을 쓰기 위해 어떤 준비를 했을까?\\n<bot> 바그너는 파우스트를 읽고 그 내용에 매료되었고, 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖게 되었다. 바그너'}]\n"
     ]
    }
   ],
   "source": [
    "print(bot_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "497d8cc6-95a8-4155-b414-9efe93811b92",
   "metadata": {
    "id": "497d8cc6-95a8-4155-b414-9efe93811b92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 바그너는 파우스트를 읽고 그 내용에 매료되었고, 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖게 되었다. 바그너는 파우스트를 읽고 그 내용에 매료되었고, 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖게 되었다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bot_text[0]['generated_text'].split(\"<usr>\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5f7a094-2fc8-4055-b1f7-337da8b1d27a",
   "metadata": {
    "id": "b5f7a094-2fc8-4055-b1f7-337da8b1d27a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sys>1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\\n<usr> 바그너가 1839년에 파우스트를 소재로 한 교향곡 작곡을 시작했다는데, 왜 이 소재에 마음이 끌렸을까요?\\n<bot> 바그너는 파우스트의 메피스토펠레스를 만나는 파우스트의 심경에 공감했기 때문입니다. 또한 바그너는 파리에서 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았고, 이것이 이 작품에 조금 영향을 끼쳤다고 합니다.\\n<usr> 작곡을 시작한 이후 작업은 어떻게 진행됐나요?\\n<bot> 작곡을 시작한 1839년부터 40년에 걸쳐 파리에서 작업을 시작했지만, 1악장을 쓴 뒤 중단하게 됐습니다. 그리고 이 간 동안 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 바쁜 시간을 보냈는데 이런 바쁜 생활이 이 곡을 잊게 한 것이 아닐까 추측되고 있습니다.\\n<usr> 그렇다면 이 작품이 연주되지 않은 이유는 무엇인가요?\\n<bot> 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였지만, 실제로는 이루어지지 않았습니다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고, 재연도 이루어졌지만, 이후에는 그대로 방치되고 말았습니다.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e424933-6f92-41cc-976d-5cf9f7d1d636",
   "metadata": {
    "id": "9e424933-6f92-41cc-976d-5cf9f7d1d636"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
